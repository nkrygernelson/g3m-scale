ckpt_path: null
seed: 42
project: g3m
tags:
- qm9
train: true
log_dir: ${oc.env:LOG_PATH}
output_dir: ${hydra:runtime.output_dir}
work_dir: ${hydra:runtime.cwd}
data_path: ${oc.env:DATA_PATH}/qm9/preprocessed
infos_path: ${data_path}/train_infos.json
data_name: qm9
atom_types_str:
- H
- C
- 'N'
- O
- F
max_num_atoms: 29
zero_cog: true
num_layers: 5
hidden_dim: 128
num_fourier_features: 16
num_rbf_features: 64
max_distance: 12.0
parameterization: x0
monitor_metric: val/molecule_stable
val_every_n_epochs: 2
datamodule:
  _target_: src_gmmm.data.datamodule.DataModule
  transform:
    _target_: torch_geometric.transforms.Compose
    transforms:
    - _target_: src_gmmm.data.transforms.FullyConnected
    - _target_: src_gmmm.data.transforms.ZeroCoG
      key: pos
  train_path: ${data_path}/train.pt
  val_path: ${data_path}/val.pt
  train_batch_size: 64
  val_batch_size: 256
  num_val_subset: 2048
  num_workers: 1
  pin_memory: true
  subset_seed: ${seed}
lit_module:
  _target_: src_gmmm.lit.module.LitModule
  model:
    _target_: src_gmmm.model.diffusion.EquivariantDiffusion
    parameterization:
      _target_: src_gmmm.model.score.EquivariantParameterization
      encoder:
        _target_: src_gmmm.nn.encoder.EquivEncoder
        hidden_dim: ${hidden_dim}
        time_embedding:
          _target_: src_gmmm.nn.layers.FourierEmbedding
          in_features: 1
          out_features: ${num_fourier_features}
        edge_embedding:
          _target_: src_gmmm.nn.layers.EdgeEmbedding
          num_rbf_features: ${num_rbf_features}
          max_distance: ${max_distance}
        num_layers: ${num_layers}
        smooth_h: false
        h_input_dim: 100
      readout:
        _target_: src_gmmm.nn.readout.DataPointReadout
        hidden_dim: ${hidden_dim}
        pred_h: false
        pred_pos: true
        zero_cog: ${zero_cog}
        parameterization: residual-pos
    diffusion_pos:
      _target_: src_gmmm.model.continuous.ContinuousDiffusion
      sde:
        _target_: src_gmmm.model.continuous.LinearLogSNRVPSDE
      parameterization: ${parameterization}
      distribution:
        _target_: src_gmmm.model.distributions.DistributionGaussian
        dim: 3
        zero_cog: ${zero_cog}
    diffusion_h: null
  metrics:
    _target_: src_gmmm.metrics.qm9.QM9Metrics
    atom_types_str: ${atom_types_str}
    max_num_atoms: ${max_num_atoms}
    json_path: ${infos_path}
  n_integration_steps: 250
  lr: 0.0002
  warm_up_steps: 100
  with_ema: true
  ema_start_step: 500
  ema_decay: 0.999
  antithetic_time_sampling: true
  loss_during_val: true
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${output_dir}/checkpoints
    filename: '{epoch:03d}'
    monitor: ${monitor_metric}
    verbose: false
    save_last: true
    save_top_k: 3
    mode: max
    auto_insert_metric_name: true
    save_weights_only: false
    every_n_epochs: ${val_every_n_epochs}
    save_on_train_epoch_end: true
  log_sampled_atoms:
    _target_: src_gmmm.utils.callback.LogSampledAtomsCallback
    dirpath: ${output_dir}/samples
    save_atoms: true
    num_log_wandb: 25
logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    save_dir: ${output_dir}
    offline: false
    id: null
    anonymous: null
    project: ${project}
    log_model: false
    prefix: ''
    group: ''
    tags: ${tags}
    job_type: ''
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: auto
  devices: 1
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  max_epochs: 1000
  check_val_every_n_epoch: ${val_every_n_epochs}
  num_sanity_val_steps: 0
